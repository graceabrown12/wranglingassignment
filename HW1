##Q1

## Read the abstract. What is this paper about?
## This paper is about the process of data cleaning. This evaluates what data should look like before finishing cleaning, introducing a criteria (each row is an observation, each column is a variable, each type of observational unit is a table).
## Read the introduction. What is the "tidy data standard" intended to accomplish?
## The tidy data standard is intended to accomplish a standardization of data cleaning, making it easier to clean data.
## Read the intro to section 2. What does this sentence mean: "Like families, tidy datasets are all alike but every messy dataset is messy in its own way." What does this sentence mean: "For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general."
## The first sentence means that messy data is more unique than clean data, making clean data easier to work with. The second sentence means that data frames are intuitive for the data creator, and makes observations much easier to interpret.
## Read Section 2.2. How does Wickham define values, variables, and observations?
## Values are numeric or categorical/strings. A variable is a collection of values that measure the same attribute or property. An observation is a collection of values that measure it.
## How is "Tidy Data" defined in section 2.3?
## Tidy data means that each variable is a column, each observation is a row, and each type of observational unit is a table. If data doesn’t fit these qualities, then it is messy data!
## Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is "melting" a dataset?
## 1.Column headers are values (like year), and not variable names. 2.Multiple variables are stored in one column. 3.Variables are stored in both rows and columns. 4.Multiple types of observational units are stored in the same table. 5.A single observational unit is stored in multiple tables. Columns are all the names of variables, rather than the values that variables take. Melting a dataset is this process of converting column-value variables into rows.
## Why, specifically, is table 11 messy but table 12 tidy and "molten"?
## Table 11 has days along the top = values. Table 12 melts days into a single variable, date. The element variable contains variable names and not values. Table 12(b) is tidy because all the entries are attributes and not variable names.
## Read Section 6. What is the "chicken-and-egg" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?
## Wickham hopes that the tidy concept will create a larger system for idea san tools for data science. He hopes that it is beyond training data scientists, and will revolutionize the way that people use these tools.

import pandas as pd
import numpy as np
import seaborn as sns

##Q2

df = pd.read_csv('./data/airbnb_hw.csv', low_memory=False)
print( df.shape, '\n')
df.head()

price = df['Price']
price.unique()

price = df['Price']
price = price.str.replace(',','') 
print( price.unique() , '\n')
price = pd.to_numeric(price,errors='coerce') 
print( price.unique() , '\n')
print( 'Total missing: ', sum( price.isnull() ) )

df = pd.read_csv('./data/sharks.csv', low_memory=False)
# df.head()
# df.columns.tolist()

df['Type'].value_counts()

type = df['Type'] # Create a temporary vector of values for the Type variable to play with

type = type.replace(['Sea Disaster', 'Boat','Boating','Boatomg'],'Watercraft') # All watercraft/boating values
type.value_counts()

type = type.replace(['Invalid', 'Questionable','Unconfirmed','Unverified','Under investigation'],np.nan) # All unclean values
type.value_counts()

df['Type'] = type # Replace the 'Type' variable with the cleaned version
del type # Destroy the temporary vector

df['Type'].value_counts()

df['Fatal (Y/N)'] = df['Fatal (Y/N)'].replace(['UNKNOWN', 'F','M','2017'],np.nan) # All unclean values
df['Fatal (Y/N)'] = df['Fatal (Y/N)'].replace('y','Y') # All unclean values
pd.crosstab(df['Type'],df['Fatal (Y/N)'],normalize='index')

df = pd.read_csv('./data/VirginiaPretrialData2017.csv', low_memory=False)
# df.head()
# df.columns.tolist()

release = df['WhetherDefendantWasReleasedPretrial']
print(release.unique(),'\n')
print(release.value_counts(),'\n')
release = release.replace(9,np.nan) # In the codebook, the 9's are "unclear"
print(release.value_counts(),'\n')
sum(release.isnull()) # 31 missing values
df['WhetherDefendantWasReleasedPretrial'] = release # Replace data column with cleaned values
del release

length = df['ImposedSentenceAllChargeInContactEvent']
type = df['SentenceTypeAllChargesAtConvictionInContactEvent']

# print( length.unique()  , '\n') # Some values are ' ', denoting missing
length = pd.to_numeric(length,errors='coerce') # Coerce to numeric
length_NA = length.isnull() # Create a missing dummy
print( np.sum(length_NA),'\n') # 9k missing values of 23k, not so good

print( pd.crosstab(length_NA, type), '\n') 

length = length.mask( type == 4, 0) 
length = length.mask( type == 9, np.nan) 

length_NA = length.isnull() 
print( pd.crosstab(length_NA, type), '\n')
print( np.sum(length_NA),'\n') 

df['ImposedSentenceAllChargeInContactEvent'] = length # Replace data with cleaned version
del length, type # Delete temporary length/type variables

